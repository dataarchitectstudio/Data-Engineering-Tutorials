{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f50c644-c7f8-4e62-b1ec-150095e6afe9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Method 1: From a list of tuples with column names\n",
    "df1 = spark.createDataFrame(\n",
    "    data=[(\"Alice\", 30), (\"Bob\", 25)],\n",
    "    schema=[\"name\", \"age\"]\n",
    ")\n",
    "display(df1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7a8771c-7f5e-497a-a30b-05ee95b9b52d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Method 2: From a list of tuples with explicit schema\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True)\n",
    "])\n",
    "df2 = spark.createDataFrame(\n",
    "    data=[(\"Charlie\", 28), (\"Diana\", 22)],\n",
    "    schema=schema\n",
    ")\n",
    "display(df2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63945a19-a5df-43de-959d-1efab4a39a85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Method 3: From a Python dictionary using .from_records()\n",
    "import pandas as pd\n",
    "\n",
    "pdf = pd.DataFrame.from_records(\n",
    "    [{\"name\": \"Eve\", \"age\": 35}, {\"name\": \"Frank\", \"age\": 40}]\n",
    ")\n",
    "df3 = spark.createDataFrame(pdf)\n",
    "display(df3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e842a1f5-12ea-46b1-8009-c7163531c8b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Method 4: From an existing table\n",
    "# (Assuming a table named 'people' exists in the metastore)\n",
    "df4 = spark.table(\"workspace.default.sales\")\n",
    "display(df4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89fd29b9-ee83-409b-a0d9-e5da4fb7b3c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Method 5: From a CSV file\n",
    "# (Assuming '/tmp/people.csv' exists in DBFS)\n",
    "\n",
    "df5 = spark.read.csv(\"/Volumes/workspace/default/tmp/country_lookup.csv\", header=True, inferSchema=True)\n",
    "display(df5)\n",
    "\n",
    "#spark.read.format(\"parquet\").load(\"/Volumes/workspace/default/tmp/country_lookup.csv\")\n",
    "#spark.write.save(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "89f0a6fb-ea10-4623-8525-5f0c6d0a3104",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "--create table tablname as select * from;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01b15445-2ebc-4793-8451-9f45f43824f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"input_text\", \"\", \"Input Text\")\n",
    "text = dbutils.widgets.get(\"input_text\")\n",
    "\n",
    "print(text)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Dataframe Creation Notebook",
   "widgets": {
    "input_text": {
     "currentValue": "",
     "nuid": "d261a63d-91c0-44a1-b60e-2c29fdcefd74",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Input Text",
      "name": "input_text",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Input Text",
      "name": "input_text",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
